FROM openjdk:8u212-jdk-slim-stretch as jdk

FROM python:3.9

USER root

# --------------------------------------------------------
# JAVA
# --------------------------------------------------------
# RUN 
RUN apt update && apt-get install -y --no-install-recommends \
  python3-launchpadlib \
  software-properties-common

COPY --from=jdk /usr/local/openjdk-8 /usr/lib/jvm/java-8-openjdk-arm64/
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64/
ENV PATH $JAVA_HOME/bin/:$PATH

# --------------------------------------------------------
# HADOOP
# --------------------------------------------------------
ARG HADOOP_VERSION=3.3.6
ENV HADOOP_URL=https://downloads.apache.org/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_PREFIX=/opt/hadoop
ENV HADOOP_CONF_DIR=/etc/hadoop
# Adding Hadoop to Path
ENV PATH $HADOOP_PREFIX/bin/:$PATH
ENV PATH $HADOOP_HOME/bin/:$PATH
#ENV PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
ENV MULTIHOMED_NETWORK=1

ENV USER=root


# Download and extract hadoop
COPY tars/hadoop-3.3.6.tar.gz /tmp/

RUN set -x \
  && mkdir -p $HADOOP_HOME  \
  && tar -xf /tmp/hadoop-3.3.6.tar.gz -C $HADOOP_HOME --strip-components 1 \
  && mkdir $HADOOP_HOME/logs \
  && rm -rf /tmp/hadoop-3.3.6.tar.gz

# RUN set -x \
#  && mkdir $HADOOP_HOME  \
#  && curl -fSL $HADOOP_URL -o /tmp/hadoop.tar.gz \
#  && tar -xf /tmp/hadoop.tar.gz -C $HADOOP_HOME --strip-components 1 \
#  && mkdir $HADOOP_HOME/logs \
#  && rm /tmp/hadoop*

RUN ln -s /opt/hadoop/etc/hadoop /etc/hadoop
RUN mkdir /hadoop-data

# Set user for HDFS and Yarn (for production probably not smart to put root)
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"

# Add JAVA_HOME to haddop-env.sh
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> \
  "$HADOOP_HOME/etc/hadoop/hadoop-env.sh"

ADD entrypoint.sh /entrypoint.sh
RUN chmod a+x /entrypoint.sh

COPY conf/core-site.xml $HADOOP_CONF_DIR/core-site.xml
COPY conf/hdfs-site.xml $HADOOP_CONF_DIR/hdfs-site.xml
COPY conf/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
COPY conf/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml

# ADD hadoop.env /hadoop.env

# https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/#distribute-authentication-key-pairs-for-the-hadoop-user
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
  chmod 600 ~/.ssh/authorized_keys

COPY ssh_config ~/.ssh/config

ADD entrypoint.sh /entrypoint.sh
RUN chmod a+x /entrypoint.sh

# Exposing a union of ports across hadoop versions
# Well known ports including ssh
# EXPOSE 0-1024 4040 7000-10100 5000-5100 50000-50200 58188 58088 58042 

EXPOSE 22 4040

ENTRYPOINT ["/bin/bash", "/entrypoint.sh"]